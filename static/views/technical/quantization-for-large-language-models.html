<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- TODO: https://developer.mozilla.org/en-US/docs/Web/HTTP/Permissions_Policy -->
        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta
            name="viewport"
            content="width=device-width, initial-scale=1.0" />
        <title>Quantization for Large Language Models</title>
        <link
            href="https://unpkg.com/boxicons@2.1.4/css/boxicons.min.css"
            rel="stylesheet" />
        <link
            rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" />
        <link href="/static/styles/style.css" rel="stylesheet" />
        <script
            data-cdn="highlight.js"
            src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    </head>
    <body>
        <!-- Router -->
        <header>
            <nav class="router">
                <a href="/">
                    <i class="bx bxs-user bx-md"></i>
                    <span class="text">Profile</span>
                </a>
                <a href="/static/views/links.html">
                    <i class="bx bxs-file bx-md"></i>
                    <span class="text">Articles</span>
                </a>
                <a class="theme-moon" data-theme="light">
                    <i class="bx bxs-moon bx-md"></i>
                    <span class="text">Dark</span>
                </a>
            </nav>
        </header>

        <noscript>
            <p>
                Please enable JavaScript to view MathJax and syntax-highlighted
                content properly.
            </p>
        </noscript>

        <!-- Application -->
        <main><h1
id="quantization-for-large-language-models">Quantization for Large
Language Models</h1>
<p>Quantization reduces memory usage and computation by representing
floating-point values with smaller, integer-based formats (such as 4-bit
or 8-bit integers). Each quantization block has a scaling factor, <span
class="math inline"><em>δ</em></span>, that maps the floating-point
range to the integer range for storage and processing efficiency.</p>
<h3 id="general-formula-for-delta-calculation">General Formula for Delta
Calculation</h3>
<p>For a signed <strong>n-bit</strong> quantization, <span
class="math inline"><em>δ</em></span> is calculated as:</p>
<p><span class="math display">$$
\delta = \frac{\text{max} - \text{min}}{2^{n-1} - 1}
$$</span></p>
<p>where:</p>
<ul>
<li><strong>max</strong> and <strong>min</strong> are the maximum and
minimum values within the block.</li>
<li><span class="math inline">2<sup><em>n</em> − 1</sup> − 1</span>
represents the maximum positive value for a signed
<strong>n-bit</strong> integer.</li>
</ul>
<p>This formula ensures that <span class="math inline"><em>δ</em></span>
scales floating-point values to fit within the signed integer range,
preserving as much information as possible.</p>
<h3 id="q8-quantization-8-bit">q8 Quantization (8-bit)</h3>
<p>For <strong>q8</strong>, values are quantized to fit within the
signed 8-bit integer range of <span
class="math inline">[−128,127]</span>.</p>
<h4 id="delta-calculation">1. Delta Calculation</h4>
<p><span class="math display">$$
\delta = \frac{\text{max} - \text{min}}{127}
$$</span></p>
<h4 id="quantization">2. Quantization</h4>
<p>To quantize a floating-point value <span
class="math inline"><em>v</em></span> within the block:</p>
<p><span class="math display">$$
q = \text{round} \left(\frac{v - \text{min}}{\delta}\right)
$$</span></p>
<ul>
<li><span class="math inline"><em>q</em></span> is the quantized 8-bit
integer value, stored in <code>elements</code> of the
<code>block_q8_0</code> structure.</li>
</ul>
<h4 id="dequantization">3. Dequantization</h4>
<p>To reconstruct an approximate value from <span
class="math inline"><em>q</em></span>:</p>
<p><span
class="math display"><em>v</em><sub>reconstructed</sub> = <em>q</em> ⋅ <em>δ</em> + min</span></p>
<h3 id="q4-quantization-4-bit">q4 Quantization (4-bit)</h3>
<p>For <strong>q4</strong>, values are mapped to fit within the signed
4-bit integer range of <span class="math inline">[−8,7]</span>. Since
true 4-bit storage is unavailable, we pack two 4-bit values into each
byte.</p>
<h4 id="delta-calculation-1">1. Delta Calculation</h4>
<p><span class="math display">$$
\delta = \frac{\text{max} - \text{min}}{7}
$$</span></p>
<h4 id="quantization-1">2. Quantization</h4>
<p>For each floating-point value <span
class="math inline"><em>v</em></span> in the block:</p>
<p><span class="math display">$$
q = \text{round} \left(\frac{v - \text{min}}{\delta}\right)
$$</span></p>
<ul>
<li>Each quantized 4-bit value is stored in the <code>nibbles</code>
array, packing two values per <code>int8_t</code> element.</li>
</ul>
<h4 id="packing-two-q4-values-in-an-int8_t-element">3. Packing Two q4
Values in an <code>int8_t</code> Element</h4>
<p>Each <code>int8_t</code> in <code>nibbles</code> holds two 4-bit
quantized values (<code>quant0</code> and <code>quant1</code>):</p>
<pre class="c"><code>quantized_nibbles[j] = (quant0 &amp; 0x0F) | (quant1 &lt;&lt; 4);</code></pre>
<ul>
<li><code>quant0</code> occupies the lower nibble (bits 0-3).</li>
<li><code>quant1</code> is shifted into the upper nibble (bits
4-7).</li>
</ul>
<h4 id="dequantization-1">4. Dequantization</h4>
<p>To approximate the original floating-point value from <span
class="math inline"><em>q</em></span>:</p>
<p><span
class="math display"><em>v</em><sub>reconstructed</sub> = <em>q</em> ⋅ <em>δ</em> + min</span></p>
<h3 id="bit-extraction-for-q4-packed-data">Bit Extraction for q4 Packed
Data</h3>
<p>To retrieve each 4-bit value from the packed <code>int8_t</code>:</p>
<pre class="c"><code>// Extract lower nibble (quant0)
quant0 = quantized_nibbles[j] &amp; 0x0F;

// Extract upper nibble (quant1)
quant1 = (quantized_nibbles[j] &gt;&gt; 4) &amp; 0x0F;</code></pre>
<h2 id="quantization-strategy-for-transformer-layers">Quantization
Strategy for Transformer Layers</h2>
<p>Quantization provides both computational efficiency and memory
savings, but it introduces a trade-off between fidelity and compression.
This section outlines <strong>layer-specific considerations</strong> for
Mistral model quantization, balancing precision needs and computational
benefits.</p>
<h3
id="quantitative-and-qualitative-effects-of-quantization">Quantitative
and Qualitative Effects of Quantization</h3>
<h4 id="quantitative-analysis">Quantitative Analysis</h4>
<p>Quantization reduces precision by mapping higher-bit floating-point
values to a smaller integer format, impacting <strong>memory
usage</strong> and <strong>computational load</strong> while enabling
efficient deployment. Key takeaways include:</p>
<ul>
<li><strong>q8 Quantization</strong>: This approach supports high
fidelity across a range of applications and model sizes, providing
significant compression with minimal performance loss. <strong>8-bit
quantization</strong> emerges as optimal for balancing quality and
efficiency, suitable for attention mechanisms, MLPs, and
inference-focused scenarios.</li>
<li><strong>q4 Quantization</strong>: While q4 quantization offers
substantial memory savings, it introduces noticeable quality
degradation, particularly in smaller models (e.g., 7B parameters and
below). Larger models (e.g., Mixtral at 56B parameters) can sustain q4
with lower fidelity loss due to greater parameter redundancy, although
minor degradations still occur.</li>
</ul>
<h4 id="qualitative-analysis">Qualitative Analysis</h4>
<p>Quantization influences model comprehension, which is inherently more
subjective:</p>
<ul>
<li><strong>Critical Layers</strong>: Embedding, output, and
normalization layers benefit from higher precision, ensuring stability
and quality representation in critical model operations.</li>
<li><strong>Attention Mechanisms (e.g., SWA and GQA)</strong>: Selective
quantization at q8 within these mechanisms balances memory efficiency
with fidelity, supporting long-range dependency modeling without
disrupting coherence.</li>
<li><strong>Model Size Considerations</strong>: Larger models experience
minor qualitative impacts from quantization, whereas smaller models tend
to show more noticeable changes in output quality, especially with q4
quantization.</li>
</ul>
<h3 id="key-insights-from-quantization-evaluations">Key Insights from
Quantization Evaluations</h3>
<ul>
<li><strong>Consistency Across Tasks</strong>: 8-bit quantized models
maintain near-identical performance to full-precision baselines,
achieving over 99% accuracy recovery across benchmarks.</li>
<li><strong>Structural and Semantic Similarity</strong>: Metrics like
ROUGE, BERTScore, and STS confirm that q8 quantization preserves core
semantic meaning and structural coherence across output, while q4 can
introduce slight variability.</li>
<li><strong>Model-Size Dependence</strong>: Larger models (e.g., 70B,
405B) exhibit resilience to quantization, maintaining high fidelity even
under q4, while smaller models require q8 for optimal performance
retention.</li>
</ul>
<h3 id="layer-specific-quantization-recommendations">Layer-Specific
Quantization Recommendations</h3>
<p>Based on the Mistral model architecture, the following layers are
identified for specific quantization strategies:</p>
<h4 id="high-precision-layers-16-bit-or-32-bit">1. High Precision Layers
(16-bit or 32-bit)</h4>
<p>These layers require higher fidelity to maintain model comprehension
and quality.</p>
<ul>
<li><strong>Embedding Layer</strong>
(<code>model.embed_tokens.weight</code>): Essential for initial token
representation, best stored in f16 or f32.</li>
<li><strong>Output Layer</strong> (<code>lm_head.weight</code>):
Critical for accurate final predictions; higher precision ensures
quality output.</li>
<li><strong>Normalization Layers</strong>
(<code>input_layernorm.weight</code>,
<code>post_attention_layernorm.weight</code>): Stabilizes the model;
retain in f16 or f32 for consistent normalization across layers.</li>
</ul>
<h4 id="mid-precision-layers-8-bit">2. Mid Precision Layers (8-bit)</h4>
<p>These layers can benefit from q8 quantization, striking a balance
between computational efficiency and quality retention.</p>
<ul>
<li><strong>Attention Mechanism Layers</strong> (<code>q_proj</code>,
<code>k_proj</code>, <code>v_proj</code>, <code>o_proj</code>):
Essential for SWA and GQA. Quantizing these with q8 maintains high
performance while providing memory savings.</li>
<li><strong>MLP Layers</strong> (<code>gate_proj</code>,
<code>up_proj</code>, <code>down_proj</code>): Dense MLP layers are
typically large and benefit significantly from q8 quantization without
major loss in fidelity.</li>
</ul>
<h4 id="low-precision-layers-4-bit">3. Low Precision Layers (4-bit)</h4>
<p>The q4 format, while not ideal for most primary layers, can be
considered for certain auxiliary components if further compression is
necessary, though care is advised to avoid performance loss.</p>
<h2 id="quantization-profiles">Quantization Profiles</h2>
<p>These quantization profiles aim to balance computational efficiency
and memory savings with fidelity across different layer types in the
Mistral model. By adapting precision levels for each layer type, these
profiles optimize model performance while retaining comprehension in
critical layers.</p>
<h3 id="overview">Overview</h3>
<p>Two main quantization profiles are provided:</p>
<ul>
<li><strong>Q8 Profile</strong>: A mid-range profile using a mix of f32,
f16, and q8 precision for a balance between quality and efficiency.</li>
<li><strong>Q4 Profile</strong>: A maximal compression profile with more
aggressive quantization, using f32, q8, and q4 to achieve substantial
memory savings with some expected quality trade-offs.</li>
</ul>
<h3 id="precision-guide-by-layer-type">Precision Guide by Layer
Type</h3>
<table style="width:100%;">
<colgroup>
<col style="width: 21%" />
<col style="width: 21%" />
<col style="width: 23%" />
<col style="width: 24%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="header">
<th>Layer Type</th>
<th>General Recommendation</th>
<th>Q8 Profile Precision</th>
<th>Q4 Profile Precision</th>
<th>Reasoning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Embedding Layer</strong></td>
<td>f32</td>
<td>f32</td>
<td>f32</td>
<td>Ensures high-quality token embeddings for initial token
representation.</td>
</tr>
<tr class="even">
<td><strong>Output Layer</strong></td>
<td>f16 or f32</td>
<td>f32</td>
<td>f32</td>
<td>Higher precision here preserves output quality, especially under q4
compression.</td>
</tr>
<tr class="odd">
<td><strong>Layer Normalization</strong></td>
<td>f32</td>
<td>f32</td>
<td>f32</td>
<td>Full precision stabilizes the model, mitigating degradation
effects.</td>
</tr>
<tr class="even">
<td><strong>Attention Projections</strong></td>
<td>f16 or q8</td>
<td>f16</td>
<td>q8</td>
<td>Balances fidelity and efficiency for SWA and GQA, supporting robust
attention mechanisms.</td>
</tr>
<tr class="odd">
<td><strong>MLP Layers</strong></td>
<td>f16 or q8</td>
<td>q8</td>
<td>q4</td>
<td>Dense layers benefit from compression; q4 reduces memory with
acceptable quality trade-offs.</td>
</tr>
<tr class="even">
<td><strong>Auxiliary Layers</strong></td>
<td>q4</td>
<td>q8</td>
<td>q4</td>
<td>Auxiliary components can be maximally compressed in q4, while q8
provides modest efficiency gains.</td>
</tr>
</tbody>
</table>
<h3 id="profile-summaries">Profile Summaries</h3>
<h4 id="q8-profile-mid-range-fidelity">Q8 Profile (Mid-Range
Fidelity)</h4>
<p>The <strong>Q8 Profile</strong> uses:</p>
<ul>
<li><strong>f32 precision</strong> for embedding, layer normalization,
and output layers to retain full fidelity in these essential
layers.</li>
<li><strong>f16 precision</strong> for attention projection layers to
reduce memory and computation for attention mechanisms without major
fidelity loss.</li>
<li><strong>q8 precision</strong> for MLP and auxiliary layers to
achieve efficient memory use while maintaining a good balance in
quality.</li>
</ul>
<h4 id="q4-profile-maximal-compression">Q4 Profile (Maximal
Compression)</h4>
<p>The <strong>Q4 Profile</strong> prioritizes memory savings with:</p>
<ul>
<li><strong>f32 precision</strong> for embedding, layer normalization,
and output layers to mitigate comprehension degradation in core
layers.</li>
<li><strong>q8 precision</strong> for attention layers to balance
quality retention with efficient compression.</li>
<li><strong>q4 precision</strong> for MLP and auxiliary layers to
maximize memory savings, a trade-off suitable for applications where
efficiency is critical over quality.</li>
</ul>
<h3 id="considerations-for-application">Considerations for
Application</h3>
<p>These profiles provide flexibility to adjust based on testing
outcomes, specific use cases, or deployment requirements. Adjustments
may be necessary to tailor fidelity and efficiency for particular model
contexts or performance targets.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Quantization is a powerful tool for optimizing memory and computation
in large language models, enabling efficient deployment without
substantially compromising quality when applied thoughtfully. This guide
has outlined a detailed approach to balancing precision and compression
across various model layers, ensuring that each layer’s fidelity aligns
with its function and impact on model comprehension.</p>
<h3 id="key-takeaways">Key Takeaways:</h3>
<ol type="1">
<li><p><strong>Delta Calculation</strong> provides a straightforward
means to map floating-point ranges to integer-based formats, with
specific considerations for 8-bit and 4-bit quantization.</p></li>
<li><p><strong>Layer-Specific Precision</strong>:</p>
<ul>
<li><strong>High-fidelity layers</strong> (embedding, normalization, and
output) retain f32 or f16 precision to ensure stability and quality
representation.</li>
<li><strong>Mid-fidelity layers</strong> (attention and MLPs) balance
compression with fidelity by utilizing q8, preserving comprehension in
attention mechanisms without excessive memory use.</li>
<li><strong>Low-fidelity auxiliary layers</strong> benefit from q4
quantization where aggressive compression is needed, especially in
larger models with inherent parameter redundancy.</li>
</ul></li>
<li><p><strong>Quantization Profiles</strong>:</p>
<ul>
<li><strong>Q8 Profile</strong> offers a balanced approach for general
use cases, combining f32, f16, and q8 to maintain high performance with
efficient memory usage.</li>
<li><strong>Q4 Profile</strong> maximizes compression with f32 for
critical layers and q8 or q4 for attention and auxiliary layers, suited
for memory-constrained deployments.</li>
</ul></li>
<li><p><strong>Quantitative and Qualitative Impacts</strong>:</p>
<ul>
<li><strong>Quantitative gains</strong> include significant reductions
in memory and computation, especially in larger models where q8 can
offer near-baseline accuracy.</li>
<li><strong>Qualitative assessments</strong> reveal that larger models
are more resilient to lower-bit quantization, while smaller models
benefit more from q8 to retain coherence and structural accuracy.</li>
</ul></li>
</ol>
<p>This approach allows flexibility for adjusting precision across model
sizes, ensuring that quantization supports both efficient inference and
robust comprehension. As future models continue to evolve, this
structured, layer-specific quantization strategy will help maintain
model quality and deployment efficiency.</p>
<hr />
<p align="center">
Copyright (C) 2024 Austin Berrio
</p></main>

        <!-- marked -->
        <script
            data-cdn="marked"
            src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"
            defer></script>

        <!-- highlight -->
        <script
            data-cdn="highlight.js"
            src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/go.min.js"></script>

        <!-- Enable Inline and Block Level Rendering-->
        <script
            data-cdn="MathJax"
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
            id="MathJax"></script>

        <!-- Enable ASCII Rendering -->
        <script
            data-cdn="MathJax"
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/input/asciimath.js"
            charset="UTF-8"></script>

        <!-- Modules -->
        <script src="/static/modules/main.mjs" type="module"></script>
        <script type="module">
            import {
                Theme,
                responsiveTables,
                setupMarkedOptions,
                setupHighlightJS,
                checkCDNDependencies
            } from '/static/modules/main.mjs';

            const theme = new Theme();

            document.addEventListener('DOMContentLoaded', () => {
                console.log('DOMContentLoaded event triggered');

                theme.init();
                responsiveTables();
                checkCDNDependencies(); // Check CDN dependencies

                if (typeof marked !== 'undefined') {
                    setupMarkedOptions();
                }

                if (typeof hljs !== 'undefined') {
                    setupHighlightJS();
                }
            });
        </script>
    </body>
</html>
